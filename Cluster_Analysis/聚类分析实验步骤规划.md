# 聚类分析实验步骤规划

> **写给完全不懂数据分析的小白：** 这份文档会用最简单的方式，像教七岁小孩一样，一步步告诉你如何完成这个实验。

---

## 📚 目录

1. [第一步：理解我们要做什么](#第一步理解我们要做什么)
2. [第二步：准备工具和环境](#第二步准备工具和环境)
3. [第三步：认识我们的数据](#第三步认识我们的数据)
4. [第四步：选择要用的算法](#第四步选择要用的算法)
5. [第五步：设计评估指标](#第五步设计评估指标)
6. [第六步：设计可视化方案](#第六步设计可视化方案)
7. [第七步：统计算法效率](#第七步统计算法效率)
8. [第八步：完整实验流程](#第八步完整实验流程)
9. [第九步：实验报告结构](#第九步实验报告结构)

---

## 第一步：理解我们要做什么

### 🎯 实验目标（用最简单的话说）

想象你有一堆彩色玻璃珠，它们散落在地上。**聚类分析**就是把这些玻璃珠按照颜色（或者大小、形状）分成几堆，让相似的珠子聚在一起。

在我们的实验中：
- **数据** = 玻璃珠（比如1000个二维点、200个消费者、8950个信用卡用户等）
- **聚类算法** = 分珠子的方法（K-means、层次聚类等）
- **评估指标** = 判断分得好不好的标准（比如：同一堆的珠子是不是真的相似？）
- **可视化** = 画图展示分堆的结果（让人一眼就能看懂）
- **效率统计** = 记录每种方法用了多长时间、多少内存

### ✅ 实验要求清单

- [ ] 至少使用4种聚类算法
- [ ] 设计评估指标，统计不同数据集下的表现
- [ ] 设计可视化方法，直观展示结果
- [ ] 统计算法效率（时间、内存、复杂度），并进行对比

---

## 第二步：准备工具和环境

### 🛠️ 需要安装的工具

就像做菜需要锅和铲子一样，做数据分析需要安装Python和一些库（库就是别人写好的工具包）。

#### 2.1 安装Python

1. 打开浏览器，访问：https://www.python.org/downloads/
2. 下载Python 3.8或更高版本
3. 安装时**记得勾选**"Add Python to PATH"（这样电脑才能找到Python）

#### 2.2 安装必需的库

打开命令行（Windows按`Win+R`，输入`cmd`，回车），然后依次输入以下命令：

```bash
# 安装数据处理库
pip install pandas numpy

# 安装机器学习库（包含聚类算法）
pip install scikit-learn

# 安装可视化库
pip install matplotlib seaborn

# 安装内存监控库
pip install memory-profiler psutil

# 安装进度条库（让程序运行时有进度显示）
pip install tqdm
```

**验证安装：**
在命令行输入 `python`，然后输入：
```python
import pandas
import sklearn
import matplotlib
print("安装成功！")
```

如果没报错，说明安装成功！

---

## 第三步：认识我们的数据

### 📊 四个数据集简介

#### 数据集1：二维点集（data-8-2-1000.txt）
- **样子**：每行两个数字，代表一个点的X坐标和Y坐标
- **数量**：1000个点
- **特点**：最简单，只有2个维度，容易画图展示
- **适合做什么**：测试算法、画散点图看聚类效果

#### 数据集2：股票数据（SP500array.csv）
- **样子**：每行490个数字，代表490家公司在某一天的股价
- **数量**：470行（470天）
- **特点**：维度很高（490维），数据量大
- **适合做什么**：测试算法在高维数据上的表现

#### 数据集3：消费者数据（Mall_Customers.csv）
- **样子**：有5列：用户ID、性别、年龄、年收入、消费得分
- **数量**：200个消费者
- **特点**：有分类特征（性别）和数值特征（年龄、收入、得分）
- **适合做什么**：客户分群，比如找出"高收入高消费"的VIP客户

#### 数据集4：信用卡数据（CC GENERAL.csv）
- **样子**：有18列，包括余额、购买频率、信用额度等
- **数量**：8950个用户
- **特点**：特征多（18个），数据量大，可能有缺失值
- **适合做什么**：用户画像分析，找出不同类型的信用卡用户

### 🔍 数据预处理步骤（必须做）

1. **读取数据**：用pandas读取CSV文件
2. **查看数据**：看看数据长什么样，有多少行、多少列
3. **处理缺失值**：如果有空值，要么删除，要么填充
4. **标准化**：把不同尺度的数据统一到同一个范围（比如年龄是0-100，收入是0-100000，需要标准化）
5. **选择特征**：决定用哪些列来做聚类（比如消费者数据可能只用年龄、收入、消费得分）

---

## 第四步：选择要用的算法

### 🎲 至少选择4种算法

#### 算法1：K-means（K均值聚类）
- **简单理解**：先随机选K个中心点，然后把每个点分到最近的中心点，再移动中心点，重复直到稳定
- **优点**：快、简单、适合球形数据
- **缺点**：需要提前知道要分几类（K值），对异常值敏感
- **适用场景**：二维点集、消费者数据

#### 算法2：层次聚类（Hierarchical Clustering）
- **简单理解**：像搭积木一样，先把最相似的两个点合并，再合并次相似的，一层层往上搭
- **优点**：不需要提前知道要分几类，可以画出树状图（叫"树状图"或"谱系图"）
- **缺点**：慢，不适合大数据
- **适用场景**：小数据集（如消费者数据200个样本）

#### 算法3：DBSCAN（密度聚类）
- **简单理解**：把密集的点聚成一类，稀疏的点当作噪声（离群点）
- **优点**：能自动找出类别数，能识别噪声点，适合不规则形状的聚类
- **缺点**：对参数敏感（需要调参）
- **适用场景**：二维点集（可能有噪声点的情况）

#### 算法4：谱聚类（Spectral Clustering）
- **简单理解**：先把数据画成图，然后对图进行切割，把连通的区域分成一类
- **优点**：适合非凸形状的聚类，效果通常不错
- **缺点**：计算复杂，需要选择相似度函数
- **适用场景**：复杂形状的数据

#### 算法5（可选）：Gaussian Mixture Model（高斯混合模型）
- **简单理解**：假设数据是由多个高斯分布（钟形曲线）混合而成的，找出这些分布
- **优点**：可以给出每个点属于每个类的概率（软聚类）
- **缺点**：计算较慢
- **适用场景**：需要概率输出的场景

#### 算法6（可选）：Mean Shift（均值漂移）
- **简单理解**：像爬山一样，每个点都朝着密度最高的方向移动，最终聚集到山顶（聚类中心）
- **优点**：不需要提前知道类别数
- **缺点**：计算慢
- **适用场景**：数据分布复杂的情况

**建议选择**：K-means、层次聚类、DBSCAN、谱聚类（这4个最经典）

---

## 第五步：设计评估指标

### 📏 评估指标的作用

就像考试有分数一样，我们需要用数字来判断聚类结果好不好。

### 🎯 推荐的评估指标

#### 指标1：轮廓系数（Silhouette Score）
- **范围**：-1 到 1
- **含义**：
  - 接近1 = 聚类效果好（同一类内的点很相似，不同类之间很不同）
  - 接近0 = 聚类效果一般
  - 接近-1 = 聚类效果差（点可能分错类了）
- **适用场景**：所有数据集
- **计算方式**：用sklearn的`silhouette_score`函数

#### 指标2：Calinski-Harabasz指数（CH指数）
- **范围**：越大越好
- **含义**：类间距离大、类内距离小 = 分数高
- **适用场景**：所有数据集
- **计算方式**：用sklearn的`calinski_harabasz_score`函数

#### 指标3：Davies-Bouldin指数（DB指数）
- **范围**：越小越好（0最好）
- **含义**：类内紧密、类间分离 = 分数低
- **适用场景**：所有数据集
- **计算方式**：用sklearn的`davies_bouldin_score`函数

#### 指标4：调整兰德指数（Adjusted Rand Index, ARI）
- **范围**：-1 到 1（如果有真实标签）
- **含义**：如果数据集有真实标签（比如已知某些用户是VIP），可以对比聚类结果和真实标签的相似度
- **适用场景**：有真实标签的数据集（我们的数据集可能没有，但可以尝试）

#### 指标5：惯性（Inertia，仅K-means）
- **范围**：越小越好
- **含义**：每个点到其聚类中心的距离平方和
- **适用场景**：K-means算法

### 📊 如何选择指标？

- **二维点集**：用轮廓系数 + CH指数 + 可视化（画图看效果）
- **股票数据**：用轮廓系数 + DB指数（高维数据）
- **消费者数据**：用轮廓系数 + CH指数 + 可视化
- **信用卡数据**：用轮廓系数 + DB指数 + CH指数

**建议**：每个数据集至少计算2-3个指标，然后对比不同算法的表现。

---

## 第六步：设计可视化方案

### 🎨 可视化的作用

一张好图胜过千言万语！可视化能让我们一眼看出聚类效果。

### 📈 推荐的可视化方法

#### 可视化1：散点图（2D数据）
- **适用**：二维点集、消费者数据（选2个特征，如年龄-收入）
- **方法**：用matplotlib画散点图，不同颜色代表不同类别
- **效果**：能直观看出点被分成了几堆

#### 可视化2：树状图/谱系图（层次聚类）
- **适用**：层次聚类算法
- **方法**：用scipy的`dendrogram`函数
- **效果**：展示聚类的层次结构

#### 可视化3：热力图（评估指标对比）
- **适用**：对比不同算法在不同数据集上的表现
- **方法**：用seaborn的`heatmap`函数
- **效果**：一眼看出哪个算法在哪个数据集上表现最好

#### 可视化4：雷达图/条形图（多指标对比）
- **适用**：对比多个评估指标
- **方法**：用matplotlib画条形图或雷达图
- **效果**：综合对比不同算法的优劣

#### 可视化5：3D散点图（3个特征）
- **适用**：消费者数据（年龄、收入、消费得分）
- **方法**：用matplotlib的3D绘图功能
- **效果**：在三维空间中展示聚类结果

#### 可视化6：降维可视化（高维数据）
- **适用**：股票数据（490维）、信用卡数据（18维）
- **方法**：先用PCA（主成分分析）降到2维，再画散点图
- **效果**：把高维数据"压缩"到2维，方便可视化

### 🎯 每个数据集的可视化方案

| 数据集 | 推荐可视化方法 |
|--------|--------------|
| 二维点集 | 散点图（直接画X-Y坐标） |
| 股票数据 | PCA降维后的散点图 + 热力图（评估指标） |
| 消费者数据 | 2D散点图（年龄-收入）+ 3D散点图（年龄-收入-消费得分） |
| 信用卡数据 | PCA降维后的散点图 + 热力图（评估指标） |

---

## 第七步：统计算法效率

### ⏱️ 效率统计包括什么？

1. **运行时间**：算法跑了多久
2. **内存占用**：算法用了多少内存
3. **算法复杂度**：理论上的时间复杂度（比如O(n²)）

### 🔧 如何统计？

#### 7.1 运行时间统计

```python
import time

# 记录开始时间
start_time = time.time()

# 运行聚类算法
clustering.fit(data)

# 记录结束时间
end_time = time.time()

# 计算运行时间（秒）
running_time = end_time - start_time
```

#### 7.2 内存占用统计

```python
from memory_profiler import profile
import psutil
import os

# 方法1：使用memory_profiler
@profile
def run_clustering():
    clustering.fit(data)

# 方法2：使用psutil
process = psutil.Process(os.getpid())
memory_before = process.memory_info().rss / 1024 / 1024  # MB
clustering.fit(data)
memory_after = process.memory_info().rss / 1024 / 1024  # MB
memory_used = memory_after - memory_before
```

#### 7.3 算法复杂度分析

| 算法 | 时间复杂度 | 空间复杂度 | 说明 |
|------|-----------|-----------|------|
| K-means | O(n×k×d×i) | O(n×d) | n=样本数，k=类别数，d=维度，i=迭代次数 |
| 层次聚类 | O(n³) | O(n²) | 最慢，不适合大数据 |
| DBSCAN | O(n log n) | O(n) | 使用索引结构时 |
| 谱聚类 | O(n³) | O(n²) | 需要计算特征值分解 |

### 📊 如何对比不同数据集规模？

可以这样做：
1. 对大数据集（如信用卡数据8950个样本）进行**采样**，生成不同规模的数据集（如100、500、1000、5000、8950）
2. 在每个规模上运行所有算法
3. 画图展示：横轴=数据规模，纵轴=运行时间/内存占用
4. 分析：哪个算法随数据规模增长最快？

---

## 第八步：完整实验流程

### 🔄 实验流程图

```
开始
  ↓
1. 读取数据
  ↓
2. 数据预处理（清洗、标准化、特征选择）
  ↓
3. 对每个数据集：
   ├─ 3.1 选择算法（至少4种）
   ├─ 3.2 运行聚类算法
   ├─ 3.3 计算评估指标
   ├─ 3.4 统计运行时间和内存
   └─ 3.5 生成可视化图表
  ↓
4. 汇总结果（制作对比表格、图表）
  ↓
5. 分析结论
  ↓
结束
```

### 📝 详细步骤

#### 步骤1：创建项目文件夹结构

```
Data_Mining/
├── dataset/                    # 数据集文件夹（已有）
│   ├── data-8-2-1000.txt
│   ├── SP500array.csv
│   ├── Mall_Customers.csv
│   └── CC GENERAL.csv
├── code/                       # 代码文件夹（新建）
│   ├── 01_data_loading.py      # 数据加载
│   ├── 02_data_preprocessing.py # 数据预处理
│   ├── 03_kmeans.py           # K-means算法
│   ├── 04_hierarchical.py      # 层次聚类
│   ├── 05_dbscan.py           # DBSCAN
│   ├── 06_spectral.py         # 谱聚类
│   ├── 07_evaluation.py       # 评估指标计算
│   ├── 08_visualization.py    # 可视化
│   ├── 09_efficiency.py       # 效率统计
│   └── 10_main.py            # 主程序（整合所有功能）
├── results/                    # 结果文件夹（新建）
│   ├── figures/               # 保存图片
│   ├── tables/                # 保存表格（CSV）
│   └── logs/                  # 保存日志
└── 聚类分析实验步骤规划.md    # 本文件
```

#### 步骤2：编写代码（按顺序）

1. **01_data_loading.py**：读取4个数据集
2. **02_data_preprocessing.py**：清洗数据、标准化、特征选择
3. **03-06_算法文件**：实现4种聚类算法
4. **07_evaluation.py**：计算评估指标
5. **08_visualization.py**：生成所有图表
6. **09_efficiency.py**：统计时间和内存
7. **10_main.py**：整合所有功能，一键运行

#### 步骤3：运行实验

```bash
# 在项目根目录下运行
python code/10_main.py
```

#### 步骤4：检查结果

- 查看`results/figures/`文件夹中的图片
- 查看`results/tables/`文件夹中的评估指标表格
- 查看控制台输出的运行时间和内存信息

---

## 第九步：实验报告结构

### 📄 报告应该包含什么？

#### 1. 封面
- 课程名称、实验名称、姓名、学号、日期

#### 2. 摘要
- 用200-300字概括：做了什么、用了什么方法、得到了什么结论

#### 3. 引言
- 背景介绍：什么是聚类分析
- 实验目的：要解决什么问题
- 数据集介绍：4个数据集的基本信息

#### 4. 相关工作/理论基础
- 简要介绍4种聚类算法的原理（不用太深入，但要说明白）

#### 5. 实验设计
- 5.1 数据预处理方法
- 5.2 算法选择及参数设置
- 5.3 评估指标设计
- 5.4 可视化方案
- 5.5 效率统计方法

#### 6. 实验结果
- 6.1 数据集1（二维点集）的结果
  - 评估指标表格
  - 可视化图片（散点图）
  - 运行时间和内存统计
- 6.2 数据集2（股票数据）的结果
  - （同上）
- 6.3 数据集3（消费者数据）的结果
  - （同上）
- 6.4 数据集4（信用卡数据）的结果
  - （同上）

#### 7. 结果分析与讨论
- 7.1 不同算法在不同数据集上的表现对比
  - 制作综合对比表格
  - 分析：哪个算法在哪个数据集上表现最好？为什么？
- 7.2 算法效率对比
  - 画图展示：数据规模 vs 运行时间
  - 画图展示：数据规模 vs 内存占用
  - 分析：哪个算法最快？哪个最慢？为什么？
- 7.3 可视化结果分析
  - 分析每张图：聚类效果如何？有没有明显的类别？
- 7.4 评估指标分析
  - 分析：哪些指标更可靠？不同指标给出的结论是否一致？

#### 8. 结论
- 总结主要发现
- 指出实验的局限性
- 提出改进方向

#### 9. 参考文献
- 列出参考的论文、书籍、网站

#### 10. 附录
- 10.1 完整代码（可以放在附录，也可以单独提交）
- 10.2 更多实验结果图片

### 📊 报告中的图表要求

- **每个数据集至少2-3张图**：
  1. 聚类结果散点图（不同算法对比）
  2. 评估指标对比图（柱状图或热力图）
  3. 效率对比图（时间、内存）

- **所有图表都要有**：
  - 标题
  - 坐标轴标签
  - 图例
  - 必要的文字说明

---

## 🎯 实验检查清单

在提交报告前，请检查：

### 代码部分
- [ ] 代码能正常运行，没有错误
- [ ] 代码有注释，逻辑清晰
- [ ] 实现了至少4种聚类算法
- [ ] 计算了评估指标
- [ ] 生成了可视化图表
- [ ] 统计了运行时间和内存

### 结果部分
- [ ] 4个数据集都做了实验
- [ ] 每个数据集都有评估指标结果
- [ ] 每个数据集都有可视化图表
- [ ] 有效率统计结果（时间、内存、复杂度）

### 报告部分
- [ ] 报告结构完整（封面、摘要、引言、方法、结果、分析、结论）
- [ ] 有理论介绍（算法原理）
- [ ] 有实验结果（表格、图表）
- [ ] 有结果分析（对比、讨论）
- [ ] 有结论
- [ ] 图表清晰、有标题和说明
- [ ] 文字流畅、逻辑清晰

---

## 💡 给小白的学习建议

### 如果遇到问题怎么办？

1. **看不懂算法原理**：
   - 先看视频教程（B站搜索"聚类分析"）
   - 再看sklearn官方文档（有例子）
   - 最后看代码实现

2. **代码报错**：
   - 先看错误信息（通常很明确）
   - 检查数据格式是否正确
   - 检查库是否安装完整
   - 搜索错误信息（Stack Overflow、CSDN）

3. **结果不理想**：
   - 检查数据预处理是否正确（标准化很重要！）
   - 尝试调整算法参数
   - 尝试不同的特征组合

4. **不知道如何分析结果**：
   - 先看可视化图表（直观）
   - 再看评估指标（定量）
   - 对比不同算法的结果

### 推荐学习资源

- **Python基础**：菜鸟教程、廖雪峰Python教程
- **机器学习**：周志华《机器学习》、李航《统计学习方法》
- **sklearn文档**：https://scikit-learn.org/stable/
- **可视化**：matplotlib官方教程、seaborn官方教程

---

## 🚀 开始实验吧！

按照这个规划，一步步来，你一定能完成这个实验！

**记住**：
- 不要着急，一步一步来
- 遇到问题就查资料、问同学
- 多做实验，多对比结果
- 写报告时逻辑要清晰

**祝你实验顺利！** 🎉

---

## 📞 需要帮助？

如果在实验过程中遇到问题，可以：
1. 查看sklearn官方文档
2. 搜索相关错误信息
3. 参考其他同学的代码（但不要直接复制）
4. 向老师或助教求助

---

**文档版本**：v1.0  
**最后更新**：2025年  
**适用对象**：数据挖掘课程学生（初学者）

